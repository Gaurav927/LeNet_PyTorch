{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =64\n",
    "train_dataset = datasets.FashionMNIST(root='./FashionMNIST/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='./FashionMNIST/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "classes = ('T-shirt/top', 'Trouser','Pullover','Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle-boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress Dress Pullover  Coat\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4],nrow=4))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In original paper input size is 32x32 , but we have taken input size as 28x28\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6,16,kernel_size=3)        \n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2,stride=2)  # original paper there is average pooling\n",
    "        self.l1 = nn.Linear(400,120)\n",
    "        self.l2 = nn.Linear(120,10)      # In original paper last layer conatins 120x84 then \n",
    "                                         # implemented some Gaussian \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = F.relu(self.pooling(self.conv1(x)))    # Original paper contains Sigmoid as activation function\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.l1(x)\n",
    "        \n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return F.log_softmax(x) # In original paper last layer was Gaussian, Here we have used Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.306845\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.232642\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.161507\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.276464\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.925961\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.690422\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.623943\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.876970\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.873818\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.629184\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.696009\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.762789\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.596089\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.617514\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.675308\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.566949\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.638600\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.414413\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.492956\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.525476\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.453688\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.627776\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.797107\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.724863\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.550358\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.563904\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.665305\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.757124\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.548110\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.582292\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.617079\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.500046\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.454103\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.593310\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.539224\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.575267\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.269067\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.485597\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.451210\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.439340\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.334836\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.635644\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.399656\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.466825\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.433148\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.646714\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.290791\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.457108\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.487424\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.551920\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.413273\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.378235\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.531225\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.637861\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.324471\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.373428\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.459395\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.447870\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.511584\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.408987\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.523218\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.518940\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.418867\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.525191\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.249069\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.358450\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.312237\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.420008\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.733843\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.304906\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.381438\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.418977\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.537075\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.324270\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.349483\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.295521\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.451712\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.240886\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.296795\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.387368\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.218760\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.476963\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.371095\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.351964\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.439137\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.301667\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.441458\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.488511\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.380056\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.307243\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.882478\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.360992\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.287544\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.274510\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.426950\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.372231\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.501891\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.312575\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.222989\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.538094\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for i ,(data,target) in enumerate(train_loader):\n",
    "        data,target = Variable(data),Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(data)\n",
    "        loss = criterion(y_pred,target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 8562/10000 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    output = model.forward(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(output, target).item()\n",
    "    # get the index of the max\n",
    "    pred = output.data.max(dim=1,keepdim=True)[1]\n",
    "    \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
